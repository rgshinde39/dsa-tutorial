Data structures
	these are the way data is stored on computer memory, knowing the best data structure for your case will help achieve efficiency
Algorithms
	Alorightm is a flow including steps to solve a given problem, here we break the problem into small parts and work on them, at last we combine the result

Recursion
	its a way of solving problems where a method calls itself for no of times
	it takes a lot of memory on stack because each method call and its results are stored on stack till last recursion call
	
when to use recursion
	when problem can be broken down into parts
	when we are not worried about how much time it takes to run our code
	when we need fast solution instead of a efficient one
	when traversing a tree
	
when to avoid recursion
	when we need to consider how much time a program takes to complete
	when we want to write program thats memory efficient

steps to write recursion
1. the actual flow
2. the exit condition
3. the constraint

programs
	factorial
	fibonacci numbers - a number is sum of last 2 numbers, series starts with 0,1
	0,1,1,2,3,5,8,13.....
	
Big O notation
	it is a language and metrics used to determine efficiency of algorithms
	time complexity: a time complexity of a function/algorithm is how its runtime varies based on input
	space complexity: how much memory an algorithm would need

possible 3 cases
best case O(n)
average case O(n LOG n)
worst case O(n2)

Big O - to find the maximum time required for algorithm execution 
Big Omega - to find the minimum time required for algorithm execution 
Big Theta - to find the average time required for algorithm execution, it will fall in range of Big O and Big Omega

consider finding a element in array of a million elements 

Big O = O(n) where n equals to 1 million
big omega O(1) at least 1 execution will be needed to find the element
big theta O(n/2) on an average half a million searches are required to find a given element

different runtime complexities
O(1) constant time - example get array element at a index
O(n) linear time - example rotate over a array
O(log n) logarithmic - example would be printing array element at every 3rd index, binary search
O(n2) quadratic - example when we iterate over a array twice, where one iteration is nested in another iteration 5*5=25 iterations
O(2n) exponential - example fibonacci program

*while comparing complexities drop constants and non dominant terms

add or multiply complexity ?
	add - when algorithm says do this and then do that, one loop after another
	multiply - when algorithm says do this while doing that - nested loop

how to measure the codes with Big O
rule 1: any assignments or if statements - O(1)
rule 2: a simple for loop(no internal loops) with length 0 to n - O(n)
rule 3: a nested for loop of the same type - O(n^2)
rule 4: a loop where the controlling parameter is divided on each step - O(log n)
rule 5: for multiple statements, we just add them up

while adding them up, ignore the constants and non dominant terms

time comp levels by dominance
O(n!) > O(2^n) > O(n^2) > O(n log n) > O(n) > O(log n), O(1)

Arrays
one dimentional - linear
multi dimentional

multi dimentional arrays are stored as one dimentional array in memory

array creating steps in memory
1. declaration - creates a reff variable in memory
2. instantiation - creates array in memory then links reff variable and first element in array, arr[xx11+0] where xx11 is the physical address of first element
3. initialization - assign values to array

when to use array
	when we need to randomly access elements
	when we need to store a large no of similar elements
when to avoid
	when we need dynamic capacity storage, unlike array's fixed capacity